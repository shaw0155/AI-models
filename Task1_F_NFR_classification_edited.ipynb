{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaw0155/AI-models/blob/main/Task1_F_NFR_classification_edited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tobhey/NoRBERT/blob/master/Code/Task1_to_3_original_Promise_NFR_dataset/Task1_F_NFR_classification.ipynb)"
      ],
      "metadata": {
        "id": "zuv3EBFhi_67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification of functional and non-functional requirements on Promise NFR Dataset"
      ],
      "metadata": {
        "id": "Vg6nFRifQ1AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook includes all code needed to train and evaluate a binary classifier for predicting whether a requirement in the Promise NFR dataset is a functional or non-functional requirement."
      ],
      "metadata": {
        "id": "CSO9nA7LmQdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: some cells are hidden and only the title is shown. To display the code, double-click the cell to switch the display mode."
      ],
      "metadata": {
        "id": "tnp6fvcFmpQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare\n",
        "Install required libraries and import packages"
      ],
      "metadata": {
        "id": "-djxLyuc-Opk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Install needed libraries {display-mode: \"form\"}\n",
        "!pip uninstall numpy\n",
        "!pip install numpy==1.23.5\n",
        "!pip install fastai==1.0.61 fastcore==1.3.29 fastprogress==1.0.3 pytorch-transformers==1.2.0 sklearn==0.0 spacy==3.6.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Would remove:\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.10\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/numpy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.14 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "7fb1e36a28654a18b0bfb6f824418913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastai==1.0.61 in /usr/local/lib/python3.10/dist-packages (1.0.61)\n",
            "Requirement already satisfied: fastcore==1.3.29 in /usr/local/lib/python3.10/dist-packages (1.3.29)\n",
            "Requirement already satisfied: fastprogress==1.0.3 in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: pytorch-transformers==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: spacy==3.6.1 in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (4.12.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (3.7.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (2.10.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (1.23.5)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (7.352.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (2.1.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (24.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from fastai==1.0.61) (0.19.0+cu121)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore==1.3.29) (24.1.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (1.35.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (4.66.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2024.5.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sklearn==0.0) (1.3.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (0.9.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (71.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.6.1) (1.2.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy==3.6.1) (0.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai==1.0.61) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai==1.0.61) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai==1.0.61) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai==1.0.61) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.1) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->fastai==1.0.61) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->fastai==1.0.61) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->fastai==1.0.61) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->fastai==1.0.61) (2024.6.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.6.1) (8.1.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fastai==1.0.61) (2.6)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.10 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.35.10)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.10.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.6.1) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai==1.0.61) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai==1.0.61) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai==1.0.61) (2024.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sklearn==0.0) (3.5.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.6.1) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fastai==1.0.61) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->fastai==1.0.61) (1.3.0)\n"
          ]
        }
      ],
      "metadata": {
        "id": "Epk5taxa99eI",
        "scrolled": false,
        "outputId": "a31ab7d5-3f9a-4e55-a7a0-d572609bd5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Import python packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel, BertConfig\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "from fastprogress import master_bar, progress_bar\n",
        "from datetime import datetime"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fr6bTWdl-XzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Check, if and what kind of GPU is used\n",
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device)/1000000\n",
        "\n",
        "def get_memory_usage_str():\n",
        "    return 'Memory usage: {:.2f} MB'.format(get_memory_usage())\n",
        "\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if cuda_available:\n",
        "    curr_device = torch.cuda.current_device()\n",
        "    print(torch.cuda.get_device_name(curr_device))\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "device"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "id": "Wtzha3q7QjjU",
        "cellView": "form",
        "outputId": "70dfdcc3-9d0f-4dfe-8db4-ecf6208ad8e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define configuration used in this experiment run"
      ],
      "metadata": {
        "id": "WJRAlPs5rRIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create config and set hyperparameters.\n",
        "One can configure:\n",
        "\n",
        "\n",
        "*   BERT model to use (model_name)\n",
        "*   Learning Rate to use (max_lr)\n",
        "*   Momentum (moms)\n",
        "*   Epoch number for training (epochs)\n",
        "*   Badge size for training (bs)\n",
        "*   Weight decay for training (weight_decay)\n",
        "*   Maximal sequence length (max_seq_len)\n",
        "*   Train size used for both test/train and train/validation split (train_size)\n",
        "*   Loss function used for training (loss_func)\n",
        "*   The random seed used for shuffling, sampling and splitting (seed)\n",
        "*   Whether, or not to use early stopping (es)\n",
        "*   The minimal delta used to indicate early stopping (min_delta)\n",
        "*   The number of epochs that need to undergo this delta to early stop training (patience)\n",
        "*   The way of folding used for this experiment (either test/train split (No), ten-fold cross validation (TenFold), or project specific folding (ProjFold))\n",
        "*   Which kind of sampling to use (either OverSampling minority class, UnderSampling majority class, or NoSampling at all)\n",
        "\n",
        "*   Which class to predict (clazz)\n",
        "\n",
        "Further one can configure, where to get the dataset from and where to save log, result and model files.\n",
        "Two booleans are provided to decide whether to:\n",
        "1. load data from Google Drive or download data from zenodo and to\n",
        "2. save the model file.\n",
        "\n"
      ],
      "metadata": {
        "id": "mOKXVgJgGtYV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "\n",
        "class Fold(Enum):\n",
        "  No = 1\n",
        "  TenFold = 2\n",
        "  ProjFold = 3\n",
        "\n",
        "class Sampling(Enum):\n",
        "  NoSampling = 1\n",
        "  UnderSampling = 2\n",
        "  OverSampling = 3\n",
        "\n",
        "config = Config(\n",
        "    num_labels = 2, # will be set automatically afterwards\n",
        "    model_name=\"bert-base-cased\", # bert_base_uncased, bert_large_cased, bert_large_uncased\n",
        "    max_lr=2e-5, # default: 2e-5\n",
        "    moms=(0.8, 0.7), # default: (0.8, 0.7); alt.(0.95, 0.85)\n",
        "    epochs=16, # 10, 16, 32, 50\n",
        "    bs=16, # default: 16\n",
        "    weight_decay = 0.01,\n",
        "    max_seq_len=128, # 50, 128\n",
        "    train_size=0.75, # 0.8\n",
        "    loss_func=nn.CrossEntropyLoss(),\n",
        "    seed=904727489, #default: 904727489, 42 (as in Dalpiaz) or None\n",
        "    es = False, # True\n",
        "    min_delta = 0.01,\n",
        "    patience = 3,\n",
        "    fold = Fold.No, # Fold.No, Fold.TenFold, Fold.ProjFold\n",
        "    sampling = Sampling.NoSampling, #Sampling.UnderSampling, Sampling.NoSampling, Sampling.OverSampling\n",
        ")\n",
        "\n",
        "clazz = 'NFR' # class to train classification on\n",
        "\n",
        "config_data = Config(\n",
        "    root_folder = '.', # where is the root folder? Keep it that way if you want to load from Google Drive\n",
        "    data_folder = '/', # where is the folder containing the datasets; relative to root\n",
        "    train_data = ['promise_nfr.csv'], # dataset file to use\n",
        "    label_column = clazz,\n",
        "    log_folder_name = '/log/',\n",
        "    log_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierPredictions_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # log-file name (make sure log folder exists)\n",
        "    result_file = clazz + '_' + Fold(config.fold).name + '_' + Sampling(config.sampling).name + '_classifierResults_' + datetime.now().strftime('%Y%m%d-%H%M') + '.txt', # result-file name (make sure log folder exists)\n",
        "    model_path = '/models/', # where is the folder for the model(s); relative to the root\n",
        "    model_name = 'NoRBERT.pkl', # what is the model name?\n",
        "    gdrive_root_folder = '/content/drive/My Drive/Code/Task1_to_3_original_Promise_NFR_dataset/', # Set this to the Google Drive path. Starts with '/content/drive/' and then usually 'My Drive/*' for the files in your Drive\n",
        "\n",
        "    orig_data_set_zip = 'https://zenodo.org/record/8347866/files/NoRBERT_RE20_Paper65.zip', # link to the data set (on zenodo). DO NOT CHANGE!\n",
        "    orig_data_zip_name = 'NoRBERT_RE20_Paper65.zip', # DO NOT CHANGE\n",
        "    orig_data_file_in_zip = 'Code/Task1_to_3_original_Promise_NFR_dataset/promise_nfr.csv', # DO NOT CHANGE\n",
        "\n",
        "    # Project split to use, either p-fold (as in Dalpiaz) or loPo\n",
        "    #project_fold = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 12, 15], [2, 5, 11], [6, 9, 14], [7, 8, 13], [2, 4, 15], [4, 7, 10] ], # p-fold\n",
        "    project_fold = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15] ], # loPo\n",
        ")\n",
        "\n",
        "load_from_gdrive = False # True, if you want to use Google Drive; else, False\n",
        "save_model = False # True, if you want to use save the model file (make sure the \"models\" folder exists)"
      ],
      "outputs": [],
      "metadata": {
        "id": "i0lgLyC6Gsnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import the dataset, first we have to either load the data set from zenodo (and unzip the needed file) or connect to our Google drive (if data should be loaded from gdrive). To connect to our Google drive, we have to authenticate the access and mount the drive."
      ],
      "metadata": {
        "id": "SVU_viFX-ezy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Prepare data loading: Init loading from Google Drive, if set in config above. Else, download the data set from zenodo (using wget) {display-mode: \"form\"}\n",
        "if load_from_gdrive:\n",
        "    from google.colab import drive\n",
        "    # Connect to drive to load the corpus from there\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    config_data.root_folder = config_data.gdrive_root_folder\n",
        "else:\n",
        "    # If the file does not exist already, download the zip and extract the needed file\n",
        "    data_path = config_data.root_folder + config_data.data_folder + config_data.train_data[0]\n",
        "    data_file = Path(data_path)\n",
        "    if not data_file.exists():\n",
        "        !wget {config_data.orig_data_set_zip}\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(config_data.orig_data_zip_name) as z:\n",
        "            with open(data_path, 'wb') as f:\n",
        "                f.write(z.read(config_data.orig_data_file_in_zip))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "OmGISBrhW-VJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define logging functions and seed generation {display-mode: \"form\"}\n",
        "def initLog():\n",
        "    logfolder = config_data.root_folder + config_data.log_folder_name\n",
        "\n",
        "    if not os.path.isdir(logfolder):\n",
        "      print(\"Log folder does not exist, trying to create folder.\")\n",
        "      try:\n",
        "        os.mkdir(logfolder)\n",
        "      except OSError:\n",
        "        print (\"Creation of the directory %s failed\" % logfolder)\n",
        "      else:\n",
        "        print (\"Successfully created the directory %s\" % logfolder)\n",
        "    logfile = logfolder + config_data.log_file\n",
        "    log_txt = datetime.now().strftime('%Y-%m-%d %H:%M') + ' ' + get_info()\n",
        "    with open(logfile, 'w') as log:\n",
        "        log.write(log_txt + '\\n')\n",
        "\n",
        "def logLine(line):\n",
        "    logfile = config_data.root_folder + config_data.log_folder_name  + config_data.log_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(line + '\\n')\n",
        "\n",
        "def logResult(result):\n",
        "    logfile = config_data.root_folder + config_data.log_folder_name + config_data.result_file\n",
        "    with open(logfile, 'a') as log:\n",
        "        log.write(get_info() + '\\n')\n",
        "        log.write(result + '\\n')\n",
        "\n",
        "def get_info():\n",
        "     model_config = 'model: {}, max_lr: {}, epochs: {}, bs: {}, train_size: {}, weight decay: {},  Seed: {}, Data: {}, Column: {}, EarlyStopping: {}:{};pat:{}'.format(config.model_name, config.max_lr, config.epochs, config.bs, config.train_size, config.weight_decay, config.seed, config_data.train_data, config_data.label_column, config.es, config.min_delta, config.patience)\n",
        "     return model_config\n",
        "\n",
        "def set_seed(seed):\n",
        "    if seed is None:\n",
        "        seed = random.randint(0, 2**31)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    return seed\n",
        "\n",
        "set_seed(config.seed)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "904727489"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "id": "er1yzLHFQq1U",
        "outputId": "f7ef24a8-8f5b-40cb-96a3-546671491b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learner"
      ],
      "metadata": {
        "id": "1Tl06zRQjlKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Create proper tokenizer for our data (adapting FastAiTokenizer to use BertTokenizer) {display-mode: \"form\"}\n",
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=512, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str):\n",
        "        \"\"\"Limits the maximum sequence length. Prepend with [CLS] and append [SEP]\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6anB63ppBAtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can create our own databunch using the tokenizer above. Notice we're passing the include_bos=False and include_eos=False options. This is to prevent fastai from adding its own SOS/EOS tokens that will interfere with BERT's SOS/EOS tokens.\n",
        "\n",
        "We can pass our own list of Preprocessors to the databunch."
      ],
      "metadata": {
        "id": "1G8rFbEEJWyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define Processors and Databunch {display-mode: \"form\"}\n",
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    \"\"\"Special Tokenizer, where we remove sos/eos tokens since we add that ourselves in the tokenizer.\"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    \"\"\"Use a custom vocabulary to match the original BERT model.\"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]\n",
        "\n",
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "              tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "              label_cols:IntsOrStrs=0, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                      TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TNRRj6jIJrp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define own BertTextClassifier class{display-mode: \"form\"}\n",
        "class BertTextClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        super(BertTextClassifier, self).__init__(config)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(model_name, config=config)\n",
        "\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "\n",
        "    def forward(self, tokens, labels=None, position_ids=None, token_type_ids=None, attention_mask=None, head_mask=None):\n",
        "        outputs = self.bert(tokens, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(dropout_output)\n",
        "\n",
        "        activation = nn.Softmax(dim=1)\n",
        "        probs = activation(logits)\n",
        "\n",
        "        return logits"
      ],
      "outputs": [],
      "metadata": {
        "id": "he_PRt9Q3eUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n"
      ],
      "metadata": {
        "id": "Ptp6NhIC_FQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "IwxQFKykzpQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define functions to load data {display-mode: \"form\"}\n",
        "def load_data(filename):\n",
        "    fpath = config_data.root_folder + config_data.data_folder + filename\n",
        "    print(fpath)\n",
        "    df = pd.read_csv(fpath, delimiter=';', header=0, encoding='utf8', names=['number', 'ProjectID', 'RequirementText', 'class', 'NFR', 'F', 'A', 'FT', 'L', 'LF', 'MN', 'O', 'PE', 'PO', 'SC', 'SE', 'US'])\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "def load_all_data(filenames):\n",
        "    df = load_data(filenames[0])\n",
        "    for i in range(1, len(filenames)):\n",
        "        df = df.append(load_data(filenames[i]))\n",
        "    return df\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "oeaTvNRTypP0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Actually load the dataset{display-mode: \"form\"}\n",
        "# load the train dataset\n",
        "df = load_all_data(config_data.train_data)\n",
        "input_col = 'RequirementText'\n",
        "# shuffle the dataset a bit and get the amount of classes\n",
        "df = df.sample(frac=1, axis=0, random_state = config.seed)\n",
        "config.num_labels = df[config_data.label_column].nunique()\n",
        "\n",
        "print(df.shape)\n",
        "print(df[config_data.label_column].value_counts())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./promise_nfr.csv\n",
            "(625, 17)\n",
            "NFR\n",
            "1    370\n",
            "0    255\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "metadata": {
        "id": "-6o6UU0tUYck",
        "outputId": "23afbed5-6249-4ef3-d0e0-37c16296c10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Create the dictionary that contains the labels along with their indices. This is useful for evaluation and similar. {display-mode: \"form\"}\n",
        "def create_label_indices(df):\n",
        "    #prepare label\n",
        "    labels = ['not_' + config_data.label_column, config_data.label_column]\n",
        "\n",
        "    #create dict\n",
        "    labelDict = dict()\n",
        "    for i in range (0, len(labels)):\n",
        "        labelDict[i] = labels[i]\n",
        "    return labelDict\n",
        "\n",
        "label_indices = create_label_indices(df)\n",
        "print(label_indices)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'not_NFR', 1: 'NFR'}\n"
          ]
        }
      ],
      "metadata": {
        "id": "TWP1X17N5tJx",
        "outputId": "1778fef8-5e45-4029-f0f9-d63ee1204674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define functions for under-/oversample dataset {display-mode: \"form\"}\n",
        "def undersample(df_trn, major_label, minor_label):\n",
        "  sample_size = sum(df_trn[config_data.label_column] == minor_label)\n",
        "  majority_indices = df_trn[df_trn[config_data.label_column] == major_label].index\n",
        "  random_indices = np.random.choice(majority_indices, sample_size, replace=False)\n",
        "  sample = df_trn.loc[random_indices]\n",
        "  sample = sample.append(df_trn[df_trn[config_data.label_column] == minor_label])\n",
        "  df_trn = sample\n",
        "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
        "  print(df_trn[config_data.label_column].value_counts())\n",
        "  return df_trn\n",
        "\n",
        "def oversample(df_trn, major_label, minor_label):\n",
        "  minor_size = sum(df_trn[config_data.label_column] == minor_label)\n",
        "  major_size = sum(df_trn[config_data.label_column] == major_label)\n",
        "  multiplier = major_size//minor_size\n",
        "  sample = df_trn\n",
        "  minority_indices = df_trn[df_trn[config_data.label_column] == minor_label].index\n",
        "  diff = major_size - (multiplier * minor_size)\n",
        "  random_indices = np.random.choice(minority_indices, diff, replace=False)\n",
        "  sample = pd.concat([df_trn.loc[random_indices], sample], ignore_index=True)\n",
        "  for i in range(multiplier - 1):\n",
        "    sample = pd.concat([sample, df_trn[df_trn[config_data.label_column] == minor_label]], ignore_index=True)\n",
        "  df_trn = sample\n",
        "  df_trn = df_trn.sample(frac=1, axis=0, random_state = config.seed)\n",
        "  print(df_trn[config_data.label_column].value_counts())\n",
        "  return df_trn"
      ],
      "outputs": [],
      "metadata": {
        "id": "wsfolsicu4tn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Function to split dataframe according to Sampling strategy and train size {display-mode: \"form\"}\n",
        "def split_dataframe(df, train_size = 0.8, random_state = None):\n",
        "    # split data into training and validation set\n",
        "    df_trn, df_valid = train_test_split(df, stratify = df[config_data.label_column], train_size = train_size, random_state = random_state)\n",
        "    # apply sample strategy\n",
        "    sizeOne = sum(df_trn[config_data.label_column] == 1)\n",
        "    sizeZero = sum(df_trn[config_data.label_column] == 0)\n",
        "    major_label = 0\n",
        "    minor_label = 1\n",
        "    if sizeOne > sizeZero:\n",
        "      major_label = 1\n",
        "      minor_label = 0\n",
        "    if config.sampling == Sampling.UnderSampling:\n",
        "      df_trn = undersample(df_trn, major_label, minor_label)\n",
        "    elif config.sampling == Sampling.OverSampling:\n",
        "      df_trn = oversample(df_trn, major_label, minor_label)\n",
        "    return df_trn, df_valid"
      ],
      "outputs": [],
      "metadata": {
        "id": "wg9tHTpplBVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictor\n"
      ],
      "metadata": {
        "id": "SClv488eQC8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Create a predictor class{display-mode: \"form\"}\n",
        "class Predictor:\n",
        "    def __init__(self, classifier):\n",
        "        self.classifier = classifier\n",
        "        self.classes = self.classifier.data.classes\n",
        "\n",
        "    def predict(self, text):\n",
        "        prediction = self.classifier.predict(text)\n",
        "        prediction_class = prediction[1]\n",
        "        return self.classes[prediction_class]"
      ],
      "outputs": [],
      "metadata": {
        "id": "qubb_Ka-C78O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and train the learner/classifier\n"
      ],
      "metadata": {
        "id": "zyVQS13d5Sft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define functions to create databunch, learner and actual classifier{display-mode: \"form\"}\n",
        "def create_databunch(config, df_trn, df_valid):\n",
        "    bert_tok = BertTokenizer.from_pretrained(config.model_name,)\n",
        "    fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n",
        "    fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\n",
        "    return BertDataBunch.from_df(\".\",\n",
        "                   train_df=df_trn,\n",
        "                   valid_df=df_valid,\n",
        "                   tokenizer=fastai_tokenizer,\n",
        "                   vocab=fastai_bert_vocab,\n",
        "                   bs=config.bs,\n",
        "                   text_cols=input_col,\n",
        "                   label_cols=config_data.label_column,\n",
        "                   collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "              )\n",
        "\n",
        "\n",
        "def create_learner(config, databunch):\n",
        "    model = BertTextClassifier(config.model_name, config.num_labels)\n",
        "\n",
        "    optimizer = partial(AdamW)\n",
        "    if config.es:\n",
        "      learner = Learner(\n",
        "        databunch, model,\n",
        "        optimizer,\n",
        "        wd = config.weight_decay,\n",
        "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
        "        loss_func=config.loss_func, callback_fns=[partial(EarlyStoppingCallback, monitor='f_beta', min_delta=config.min_delta, patience=config.patience)]\n",
        "      )\n",
        "    else:\n",
        "      learner = Learner(\n",
        "        databunch, model,\n",
        "        optimizer,\n",
        "        wd = config.weight_decay,\n",
        "        metrics=FBeta(beta=1), #accuracy, (metric to optimize on)\n",
        "        loss_func=config.loss_func,\n",
        "      )\n",
        "\n",
        "    return learner\n",
        "\n",
        "# Create the classifier\n",
        "def create_classifier(config, df):\n",
        "  df_trn, df_valid = split_dataframe(df, train_size = config.train_size, random_state = config.seed)\n",
        "  databunch = create_databunch(config, df_trn, df_valid)\n",
        "\n",
        "  return create_learner(config, databunch)"
      ],
      "outputs": [],
      "metadata": {
        "id": "T83UogVz5XJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define predict loop {display-mode: \"form\"}\n",
        "def predict_and_log_result(classifier, df_eval):\n",
        "  predictor = Predictor(classifier)\n",
        "  flat_predictions, flat_true_labels = [], []\n",
        "  column_index = df_eval.columns.get_loc(config_data.label_column)\n",
        "  for row in progress_bar(df_eval.itertuples(), total=len(df_eval)):\n",
        "      class_text = row.RequirementText\n",
        "      class_label = row[column_index+1]\n",
        "      flat_true_labels.append(class_label)\n",
        "      prediction = predictor.predict(class_text)\n",
        "      flat_predictions.append(prediction)\n",
        "\n",
        "      log_text = 'PID: {}, {}, {} -> {}'.format(row.ProjectID, class_text, label_indices.get(class_label), label_indices.get(prediction))\n",
        "      logLine(log_text)\n",
        "\n",
        "  # get labels in correct order\n",
        "  target_names = []\n",
        "  test_labels = unique_labels(flat_true_labels, flat_predictions)\n",
        "  test_labels = np.sort(test_labels)\n",
        "  for x in test_labels:\n",
        "    target_names.append(label_indices.get(x))\n",
        "\n",
        "  result = classification_report(flat_true_labels, flat_predictions, target_names=target_names, digits = 5)\n",
        "  logResult(result)\n",
        "  print(result)\n",
        "  return flat_predictions, flat_true_labels"
      ],
      "outputs": [],
      "metadata": {
        "id": "iY9hh5FPo20_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define train and test loop{display-mode: \"form\"}\n",
        "def train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results):\n",
        "  classifier = create_classifier(config, df_train)\n",
        "  # Train the classifier on train set\n",
        "  print(classifier.fit_one_cycle(config.epochs, max_lr=config.max_lr, moms=config.moms, wd=config.weight_decay))\n",
        "  #Predict on test set\n",
        "  flat_predictions, flat_true_labels = predict_and_log_result(classifier, df_eval)\n",
        "  overall_flat_predictions.extend(flat_predictions)\n",
        "  overall_flat_true_labels.extend(flat_true_labels)\n",
        "  test_labels = df_eval[config_data.label_column].unique()\n",
        "  test_labels = np.sort(test_labels)\n",
        "  results.extend(precision_recall_fscore_support(flat_true_labels, flat_predictions, labels = test_labels))\n",
        "  return classifier, overall_flat_predictions, overall_flat_true_labels, results"
      ],
      "outputs": [],
      "metadata": {
        "id": "IZBH8aOZ1QfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Decide how to fold and train the classifier {display-mode: \"form\"}\n",
        "overall_flat_predictions, overall_flat_true_labels, results = [], [], []\n",
        "initLog()\n",
        "\n",
        "if config.fold == Fold.TenFold: # Use Stratified ten fold cross validation\n",
        "  skf = StratifiedKFold(n_splits=10)\n",
        "  fold_number = 1\n",
        "  for train, test in skf.split(df, df[config_data.label_column]):\n",
        "    df_train = df.iloc[train]\n",
        "    df_eval = df.iloc[test]\n",
        "    log_text = '/////////////////////// Fold: {} of {} /////////////////////////////'.format(fold_number,10)\n",
        "    logLine(log_text)\n",
        "    classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
        "    fold_number = fold_number + 1\n",
        "elif config.fold == Fold.ProjFold: # Use project specific fold as described in config_data\n",
        "  for k in config_data.project_fold:\n",
        "    test = df.loc[df['ProjectID'].isin(k)].index\n",
        "    train = df.loc[~df['ProjectID'].isin(k)].index\n",
        "    df_train = df.loc[train]\n",
        "    df_eval = df.loc[test]\n",
        "    log_text = '/////////////////////// Test-Projects: {} /////////////////////////////'.format(k)\n",
        "    logLine(log_text)\n",
        "    classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
        "else: # Use train/test split\n",
        "  df_train, df_eval = train_test_split(df,stratify=df[config_data.label_column], train_size=config.train_size, random_state= config.seed)\n",
        "  classifier, overall_flat_predictions, overall_flat_true_labels, results = train_and_predict(df_train, df_eval, overall_flat_predictions, overall_flat_true_labels, results)\n",
        "\n",
        "get_memory_usage_str()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/fastai/text/data.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([],dtype=np.int)\n",
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "100%|| 433/433 [00:00<00:00, 1241376.37B/s]\n",
            "100%|| 435779157/435779157 [00:15<00:00, 28848869.08B/s]\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_transformers/modeling_utils.py:539: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>f_beta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.674514</td>\n",
              "      <td>0.682257</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671778</td>\n",
              "      <td>0.631655</td>\n",
              "      <td>0.783626</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.621329</td>\n",
              "      <td>0.660826</td>\n",
              "      <td>0.788571</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.583262</td>\n",
              "      <td>0.418253</td>\n",
              "      <td>0.883117</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.480430</td>\n",
              "      <td>0.323586</td>\n",
              "      <td>0.870229</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.373863</td>\n",
              "      <td>0.204435</td>\n",
              "      <td>0.951049</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.265604</td>\n",
              "      <td>0.248979</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.183949</td>\n",
              "      <td>0.327079</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.132949</td>\n",
              "      <td>0.220737</td>\n",
              "      <td>0.945206</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.094596</td>\n",
              "      <td>0.268397</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.069073</td>\n",
              "      <td>0.197032</td>\n",
              "      <td>0.951724</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.051753</td>\n",
              "      <td>0.221173</td>\n",
              "      <td>0.951724</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.036691</td>\n",
              "      <td>0.254994</td>\n",
              "      <td>0.945206</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.030305</td>\n",
              "      <td>0.250871</td>\n",
              "      <td>0.945206</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.025280</td>\n",
              "      <td>0.246285</td>\n",
              "      <td>0.945206</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.024244</td>\n",
              "      <td>0.245755</td>\n",
              "      <td>0.945206</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/fastai/text/data.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([],dtype=np.int)\n",
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/fastai/text/data.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([],dtype=np.int)\n",
            "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='157' class='' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [157/157 00:02&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     not_NFR    0.96364   0.82812   0.89076        64\n",
            "         NFR    0.89216   0.97849   0.93333        93\n",
            "\n",
            "    accuracy                        0.91720       157\n",
            "   macro avg    0.92790   0.90331   0.91204       157\n",
            "weighted avg    0.92130   0.91720   0.91598       157\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Memory usage: 1327.43 MB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {
        "id": "sqGyLEubBw60",
        "outputId": "88248773-00bb-4754-f966-c865dd9c8f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Define function to calculate averaged metric results {display-mode: \"form\"}\n",
        "def calcAverageMetrics(results):\n",
        "  precisions, recalls, fscores = [], [], []\n",
        "  for i in range(int(len(results)/4)):\n",
        "    precisions.append(results[i*4])\n",
        "    recalls.append(results[i*4+1])\n",
        "    fscores.append(results[i*4+2])\n",
        "  precision = [0]*len(precisions[0])\n",
        "  recall = [0]*len(recalls[0])\n",
        "  fscore = [0]*len(fscores[0])\n",
        "  for i in range(len(precisions)):\n",
        "    precision = precision + precisions[i]\n",
        "    recall = recall + recalls[i]\n",
        "    fscore = fscore + fscores[i]\n",
        "  precision = precision / int(len(results)/4)\n",
        "  recall = recall / int(len(results)/4)\n",
        "  fscore = fscore / int(len(results)/4)\n",
        "  return precision, recall, fscore"
      ],
      "outputs": [],
      "metadata": {
        "id": "YPCIH1oPgCAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Display and log overall evaluation results {display-mode: \"form\"}\n",
        "target_names = []\n",
        "test_labels = df_eval[config_data.label_column].unique()\n",
        "\n",
        "test_labels = np.sort(test_labels)\n",
        "for x in test_labels:\n",
        "  target_names.append(label_indices.get(x))\n",
        "\n",
        "print('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
        "logResult('/////////////////////// Aggregated Predictions Result /////////////////////////////')\n",
        "result = classification_report(overall_flat_true_labels, overall_flat_predictions, target_names=target_names, digits = 5)\n",
        "logResult(result)\n",
        "print(result)\n",
        "print('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
        "logResult('/////////////////////// Averaged Metrics Result /////////////////////////////')\n",
        "precision, recall, fscore = calcAverageMetrics(results)\n",
        "print(\"              precision    recall  f1-score\")\n",
        "logResult(\"              precision    recall  f1-score\")\n",
        "for i in range(len(precision)):\n",
        "  print('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))\n",
        "  logResult('{:<14}'.format(target_names[i]) + '  {:.5f}'.format(precision[i]) + '   {:.5f}'.format(recall[i]) + '   {:.5f}'.format(fscore[i]))\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/////////////////////// Aggregated Predictions Result /////////////////////////////\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     not_NFR    0.96364   0.82812   0.89076        64\n",
            "         NFR    0.89216   0.97849   0.93333        93\n",
            "\n",
            "    accuracy                        0.91720       157\n",
            "   macro avg    0.92790   0.90331   0.91204       157\n",
            "weighted avg    0.92130   0.91720   0.91598       157\n",
            "\n",
            "/////////////////////// Averaged Metrics Result /////////////////////////////\n",
            "              precision    recall  f1-score\n",
            "not_NFR         0.96364   0.82812   0.89076\n",
            "NFR             0.89216   0.97849   0.93333\n"
          ]
        }
      ],
      "metadata": {
        "id": "5oGs3EQjnHu-",
        "outputId": "7b51cf7a-905a-411a-cad9-1a454fed44c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "UeoCn0Bs-Wds"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Save the model along with its config\n",
        "def create_model_name():\n",
        "    name = 'NoRBERT_{clasz}_e{epochs}_{sampling}'.format(clasz=clazz, epochs=str(config.epochs),sampling=Sampling(config.sampling).name)\n",
        "    return name\n",
        "\n",
        "def save_config(model_save_path, model_name):\n",
        "    settings = ''\n",
        "    for item in config.__dict__:\n",
        "        value = config[item]\n",
        "        setting = '{item}={value},\\n'.format(item=item, value=value)\n",
        "        settings += setting\n",
        "    save_path = model_save_path + model_name + '.config'\n",
        "    with open(save_path, 'w', encoding='utf-8') as out:\n",
        "        out.write(settings)\n",
        "\n",
        "if save_model:\n",
        "  model_name = create_model_name()\n",
        "  model_save_path = config_data.root_folder + config_data.model_path\n",
        "  if not os.path.isdir(model_save_path):\n",
        "    print(\"Models folder does not exist, trying to create folder.\")\n",
        "    try:\n",
        "      os.mkdir(model_save_path)\n",
        "    except OSError:\n",
        "      print (\"Creation of the directory %s failed\" % model_save_path)\n",
        "    else:\n",
        "      print (\"Successfully created the directory %s\" % model_save_path)\n",
        "  save_config(model_save_path, model_name)\n",
        "  model_save_file = model_save_path + model_name + '.pkl'\n",
        "  classifier.export(file = model_save_file)"
      ],
      "outputs": [],
      "metadata": {
        "id": "DXTWGILJ4kJx",
        "cellView": "form"
      }
    }
  ]
}